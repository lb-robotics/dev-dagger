{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebfdb09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5be6936f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_data = MNIST(root='~/data/mnist', download=True, train=True)\n",
    "train_x, train_y = mnist_train_data.data, mnist_train_data.targets\n",
    "mnist_test_data = MNIST(root='~/data/mnist', download=True, train=False)\n",
    "test_x, test_y = mnist_test_data.data, mnist_test_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1db67b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.float()\n",
    "test_y = test_y.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "22526e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.float().view(train_x.size(0), -1)\n",
    "test_x = test_x.float().view(test_x.size(0), -1)\n",
    "train_x_std = train_x.std(dim=-2) + 1e-4\n",
    "train_x_mean = train_x.mean(dim=-2)\n",
    "train_x = (train_x - train_x_mean) / train_x_std\n",
    "test_x = (test_x - train_x_mean) / train_x_std\n",
    "avg_norm = train_x.norm(dim=-1).mean()\n",
    "\n",
    "train_x = train_x / avg_norm\n",
    "test_x = test_x / avg_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "1ffa2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0498e73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 784])\n"
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        # Consider using BatchNorm2d as final layer of ConvNet too.\n",
    "        \n",
    "#         feature_extractor = torch.nn.Sequential(\n",
    "#             torch.nn.Conv2d(1, 32, kernel_size=(3, 3), padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=(2, 2)),  # 14 x 14\n",
    "#             torch.nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.MaxPool2d(kernel_size=(2, 2)),  # 64 x 7 x 7\n",
    "#             torch.nn.Flatten(),\n",
    "#             torch.nn.Linear(64 * 7 * 7, 10),\n",
    "#             torch.nn.BatchNorm2d(10),\n",
    "#         )\n",
    "\n",
    "#         feature_extractor = torch.nn.Sequential(\n",
    "#             torch.nn.Flatten(),\n",
    "#             torch.nn.Linear(28*28, 64),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.Linear(64, 32),\n",
    "#             torch.nn.ReLU(),\n",
    "#             torch.nn.BatchNorm2d(32),\n",
    "#         )\n",
    "\n",
    "#         feature_extractor = torch.nn.Sequential(\n",
    "#             torch.nn.Flatten(),\n",
    "#             torch.nn.BatchNorm2d(768),\n",
    "#         )\n",
    "\n",
    "        # TODO: don't hardcode image shapes maybe\n",
    "        inducing_points = feature_extractor(inducing_points.view(inducing_points.size(-2), 1, 28, 28))\n",
    "        print(inducing_points.shape)\n",
    "        # inducing points should now be m x 10\n",
    "        \n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        from IPython.core.debugger import set_trace\n",
    "        set_trace()\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.feature_extractor(x.view(x.size(-2), 1, 28, 28))\n",
    "        return super().__call__(x)\n",
    "\n",
    "num_inducing = 1024  # Can lower this if you want it to be faster\n",
    "inducing_points = train_x[:num_inducing, :]\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "46da24ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cba37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e40f4d6eed241e9a2e537b9ccab984d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fec4395d61cd4dfc8c110e4c28665c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-259-af678a9d007a>\u001b[0m(49)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     47 \u001b[0;31m        \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     48 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 49 \u001b[0;31m        \u001b[0mmean_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m        \u001b[0mcovar_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcovar_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mgpytorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultivariateNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovar_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> x\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<CatBackward>)\n",
      "ipdb> x.sum()\n",
      "tensor(-159.9819, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "ipdb> x.shape\n",
      "torch.Size([1152, 784])\n",
      "ipdb> x.norm(-1)\n",
      "tensor(0., device='cuda:0', grad_fn=<NormBackward1>)\n",
      "ipdb> x.norm(dim=-1)\n",
      "tensor([1.3266, 0.8598, 2.0228,  ..., 0.9004, 2.4091, 0.9438], device='cuda:0',\n",
      "       grad_fn=<CopyBackwards>)\n",
      "ipdb> x.norm(dim=-1).mean()\n",
      "tensor(1.0168, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# PLL is like VariationalELBO, but often gives better calibrated results (see https://arxiv.org/pdf/1910.07123.pdf)\n",
    "# Can only use PLL for regression\n",
    "mll = gpytorch.mlls.PredictiveLogLikelihood(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "epochs_iter = tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        # TODO: Use pinned memory etc etc to make the next two lines fast\n",
    "        x_batch = x_batch.cuda()\n",
    "        y_batch = y_batch.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "21c55744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e011fe4dbcf34df7a9aa07c1bc006943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_means = []\n",
    "pred_vars = []\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "with torch.no_grad():\n",
    "    for x_batch_test, y_batch_test in tqdm(test_loader):\n",
    "        x_batch_test = x_batch_test.cuda()\n",
    "        pred = likelihood(model(x_batch_test))\n",
    "        pred_means.append(pred.mean.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0049f24c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6770)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean((torch.round(torch.cat(pred_means)) - test_y).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea202018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
